{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7114c1-7713-452f-bb04-5d85e4fdea09",
   "metadata": {},
   "source": [
    "# ResNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aba873-7517-4070-bb23-c08467622857",
   "metadata": {},
   "source": [
    "**What is ResNet?**\n",
    "\n",
    "ResNet, or Residual Network, is a type of deep learning model that revolutionized computer vision by enabling the training of very deep networks. It was introduced in 2015 by Kaiming He and his colleagues, and it won the ImageNet competition with a significant performance boost. ResNet models are widely used for tasks like image classification, object detection, and more.\n",
    "\n",
    "\n",
    "**The Problem with Deep Neural Networks**\n",
    "\n",
    "Before ResNet, researchers found that as they made neural networks deeper (i.e., with more layers), their performance often got worse. This was due to a phenomenon called the vanishing gradient problem, where the gradients used in backpropagation to update the weights become very small, preventing the weights from updating effectively in the deeper layers.\n",
    "\n",
    "Another issue was that deeper networks often suffered from degradation, where adding more layers led to higher training error, even if the model was more capable in theory. This contradicted the idea that deeper networks should be able to learn more complex patterns.\n",
    "\n",
    "**The Key Idea Behind ResNet: Residual Learning**\n",
    "\n",
    "ResNet addresses these challenges by introducing the concept of residual learning. The key idea is to make the network learn a residual function $F(x)=H(x)-x$, rather than trying to learn the original mapping $H(x)$.\n",
    "$H(x)$ - is desired output.\n",
    "$x$ - is the input.\n",
    "The network learns the difference between the input and the desired output (the \"residual\"). The **residual block** in ResNet introduces a shortcut or \"skip connection\" that bypasses one or more layers, allowing the original input $x$ to be directly added to the output of the residual function $F(x)$.\n",
    "\n",
    "**Why Skip Connections Work**\n",
    "* Easier to Train: By using skip connections, ResNet allows gradients to flow more directly through the network, making it easier to train very deep networks without suffering from vanishing gradients.\n",
    "* Helps Prevent Degradation: Since skip connections allow the original input to be preserved, the model can avoid the problem of degradation. If deeper layers don't learn useful features, the network can still fall back on the input data.\n",
    "* Flexibility in Layer Depth: Adding more layers doesn't necessarily make performance worse because the skip connections make it possible to pass information effectively through all layers.\n",
    "\n",
    "**Structure of a Residual Block**\n",
    "\n",
    "A residual block typically consists of:\n",
    "* Two or more convolutional layers: These layers learn features from the input data.\n",
    "* Batch normalization and ReLU activation: These operations help stabilize and speed up training.\n",
    "* Skip connection (shortcut): The original input is added to the output of the convolutional layers.\n",
    "The skip connection can be a simple identity mapping (when the input and output dimensions match) or involve a linear transformation (such as a convolution) if dimensions differ.\n",
    "\n",
    "**ResNet Variants**\n",
    "\n",
    "ResNet comes in different depths, like ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152, where the number indicates the total number of layers in the network. ResNet-50 and deeper variants typically use bottleneck blocks, where a 1x1 convolution reduces the number of channels before applying a 3x3 convolution, followed by another 1x1 convolution to restore the channel size. This reduces computational cost while retaining the ability to learn complex features.\n",
    "\n",
    "**Why ResNet Works**\n",
    "\n",
    "ResNet's success can be attributed to several key factors:\n",
    "* Residual Learning Simplifies Optimization: By learning residual functions, the network optimizes the residual mappings, which are usually easier to learn, especially in deeper networks.\n",
    "* Skip Connections Mitigate Vanishing Gradient Problems: These connections enable gradients to flow backward more effectively during backpropagation, ensuring that even the earlier layers receive meaningful gradient updates.\n",
    "* Deeper Networks Capture More Complex Features: Because ResNet solves the issues that made deep networks difficult to train, it can successfully learn better representations in very deep architectures.\n",
    "\n",
    "**Key Differences Compared to Traditional Networks**\n",
    "* Skip Connections: The main difference is the presence of skip connections in ResNet, which directly pass the input to deeper layers.\n",
    "* Residual Learning: Instead of learning the full mapping, ResNet learns the residual function, which simplifies training.\n",
    "* Deeper Networks Become Practical: While earlier networks struggled beyond 20-30 layers, ResNet can have hundreds of layers (e.g., ResNet-152) without performance degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f55fdd-cdc9-40b1-9f50-dc426458a5d9",
   "metadata": {},
   "source": [
    "![ResNet18 architecture](figures/Structure-of-a-ResNet-18-architecture-223748732.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3b264c-ae0a-4b65-82a4-3b6232bde8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6f70ec-3e9b-4e65-ae38-275ad75842eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"A basic building block for ResNet models.\n",
    "\n",
    "    This block consists of two convolutional layers with a residual (skip) connection.\n",
    "    It can optionally include a downsampling layer to match the dimensions\n",
    "    when the input and output sizes are different.\n",
    "\n",
    "    The residual connection helps to mitigate the vanishing gradient problem\n",
    "    by providing a shortcut for the gradient to flow back through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # Expansion factor (used for bottleneck blocks in deeper ResNets)\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        \"\"\"Initialize the BasicBlock.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            stride (int, optional): Stride for the first convolutional layer. Defaults to 1.\n",
    "            downsample (callable, optional): Downsampling layer to match dimensions. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer:\n",
    "        # - 3x3 convolution\n",
    "        # - Applies stride (could be >1 for downsampling)\n",
    "        # - Padding=1 to maintain spatial dimensions when stride=1\n",
    "        # - No bias since BatchNorm2d handles the bias term\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        # Batch normalization layer after the first convolution\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second convolutional layer:\n",
    "        # - 3x3 convolution\n",
    "        # - Stride=1\n",
    "        # - Padding=1 to maintain spatial dimensions\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        # Batch normalization layer after the second convolution\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Optional downsampling layer to adjust dimensions of the residual connection\n",
    "        # If the input and output dimensions differ, we need to downsample the input (identity)\n",
    "        # to match the output dimensions before adding them together\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the BasicBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (N, in_channels, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (N, out_channels, H_out, W_out)\n",
    "        \"\"\"\n",
    "        # Save the input tensor for the residual (skip) connection\n",
    "        identity = x\n",
    "\n",
    "        # First layer operations:\n",
    "        out = self.conv1(x)    # Apply first convolution\n",
    "        out = self.bn1(out)    # Apply batch normalization\n",
    "        out = self.relu(out)   # Apply ReLU activation\n",
    "\n",
    "        # Second layer operations:\n",
    "        out = self.conv2(out)  # Apply second convolution\n",
    "        out = self.bn2(out)    # Apply batch normalization\n",
    "\n",
    "        # Apply downsampling to the identity (if required)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)  # Adjust dimensions of identity\n",
    "\n",
    "        # Add the identity (residual connection) to the output\n",
    "        out += identity\n",
    "\n",
    "        # Apply ReLU activation to the result\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet model class.\n",
    "\n",
    "    This class defines the architecture for ResNet-18 (or other versions depending on the layers).\n",
    "    It uses BasicBlocks to build the network.\n",
    "\n",
    "    The ResNet architecture introduces residual connections that allow\n",
    "    training of very deep networks by mitigating the vanishing gradient problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        \"\"\"Initialize the ResNet model.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Block type to use (BasicBlock or Bottleneck).\n",
    "            layers (list): Number of blocks to use in each of the four layers.\n",
    "            num_classes (int, optional): Number of classes for classification. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64  # Number of channels for the first convolutional layer\n",
    "\n",
    "        # Initial convolutional layer:\n",
    "        # - Input channels: 3 (RGB images)\n",
    "        # - Output channels: self.in_channels (64)\n",
    "        # - Kernel size: 7x7\n",
    "        # - Stride: 2 (downsamples the input)\n",
    "        # - Padding: 3 (to maintain spatial dimensions when considering kernel size and stride)\n",
    "        # - Bias is False since we're using batch normalization\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        # Batch normalization layer after the initial convolution\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Max pooling layer:\n",
    "        # - Kernel size: 3x3\n",
    "        # - Stride: 2 (further downsampling)\n",
    "        # - Padding: 1\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Build the four residual layers, each potentially changing the number of channels and the spatial dimensions\n",
    "        # Layer1: Output channels = 64, layers[0] blocks\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])   # No downsampling in Layer1\n",
    "        # Layer2: Output channels = 128, layers[1] blocks, stride=2 (downsampling)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        # Layer3: Output channels = 256, layers[2] blocks, stride=2\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        # Layer4: Output channels = 512, layers[3] blocks, stride=2\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # Adaptive average pooling:\n",
    "        # - Output size is (1,1), so regardless of input size, we get a 1x1 feature map\n",
    "        # - This allows the model to handle variable input image sizes\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Fully connected (linear) layer for classification:\n",
    "        # - Input features: 512 * block.expansion\n",
    "        #   (expansion is 1 for BasicBlock, so input features = 512)\n",
    "        # - Output features: num_classes\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # Initialize weights for the layers:\n",
    "        # - Convolutional layers are initialized using Kaiming He initialization\n",
    "        # - BatchNorm layers have weights initialized to 1 and biases to 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Kaiming normal initialization for convolutional layers with ReLU activation\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # Initialize BatchNorm weights to 1 (no scaling) and biases to 0 (no shift)\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        \"\"\"Create one of the four layers of the ResNet model consisting of multiple blocks.\n",
    "\n",
    "        Args:\n",
    "            block (nn.Module): Block type to use (BasicBlock or Bottleneck).\n",
    "            out_channels (int): Number of output channels for the blocks in this layer.\n",
    "            blocks (int): Number of blocks to include in this layer.\n",
    "            stride (int, optional): Stride for the first block in this layer. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container of the blocks forming the layer.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        # Check if we need to downsample the input (identity) to match the output dimensions:\n",
    "        # This is required when:\n",
    "        # - The stride is not 1 (the spatial dimensions will change)\n",
    "        # - The number of input channels does not match the number of output channels * expansion\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            # Define the downsampling layer:\n",
    "            # - 1x1 convolution to adjust the number of channels\n",
    "            # - Stride as specified to adjust the spatial dimensions\n",
    "            # - Bias is False since we're using batch normalization\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # First block in the layer:\n",
    "        # - May include downsampling if stride != 1 or channel dimensions change\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        # Update the number of input channels for the next blocks\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        # Remaining blocks in the layer:\n",
    "        for _ in range(1, blocks):\n",
    "            # For subsequent blocks, stride=1 and downsample=None\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        # Return a sequential container of the blocks\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the ResNet model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (N, 3, H, W), where N is batch size.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (N, num_classes)\n",
    "        \"\"\"\n",
    "        # Initial layers:\n",
    "        x = self.conv1(x)      # Apply initial convolution (reduces spatial dimensions due to stride=2)\n",
    "        x = self.bn1(x)        # Apply batch normalization\n",
    "        x = self.relu(x)       # Apply ReLU activation\n",
    "        x = self.maxpool(x)    # Apply max pooling (further reduces spatial dimensions)\n",
    "\n",
    "        # Residual layers:\n",
    "        x = self.layer1(x)     # Pass through Layer1 (spatial dimensions unchanged)\n",
    "        x = self.layer2(x)     # Pass through Layer2 (spatial dimensions reduced by half)\n",
    "        x = self.layer3(x)     # Pass through Layer3 (spatial dimensions reduced by half)\n",
    "        x = self.layer4(x)     # Pass through Layer4 (spatial dimensions reduced by half)\n",
    "\n",
    "        # Adaptive average pooling:\n",
    "        x = self.avgpool(x)    # Reduce spatial dimensions to 1x1\n",
    "        # Flatten the tensor:\n",
    "        # - From shape (N, C, 1, 1) to (N, C)\n",
    "        x = torch.flatten(x, 1)  # Flatten starting from dimension 1 (exclude batch dimension)\n",
    "        x = self.fc(x)         # Fully connected layer for classification\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(num_classes=2):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): Number of classes for classification. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        ResNet: ResNet-18 model instance.\n",
    "    \"\"\"\n",
    "    # ResNet-18 uses BasicBlock and layers=[2,2,2,2], meaning:\n",
    "    # - There are 2 BasicBlocks in each of the four layers.\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb0040a-611d-42f6-8530-a795ea800ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e153eef-56fb-4c3c-9d18-f0bf733cc203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497d73d-c306-4298-ab4a-f47901f5cce8",
   "metadata": {},
   "source": [
    "If the version above is difficult to read, here we have an unrolled version of ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8725d8c-7552-456a-adb6-e576478d7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ResNet18_unrolled(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18_unrolled, self).__init__()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # First residual block (Layer 1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_1   = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_2   = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_3   = nn.BatchNorm2d(64)\n",
    "        self.conv2_4 = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2_4   = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second residual block (Layer 2)\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3_1   = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3_2   = nn.BatchNorm2d(128)\n",
    "        self.downsample3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.conv3_3 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3_3   = nn.BatchNorm2d(128)\n",
    "        self.conv3_4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3_4   = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Third residual block (Layer 3)\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn4_1   = nn.BatchNorm2d(256)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4_2   = nn.BatchNorm2d(256)\n",
    "        self.downsample4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.conv4_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4_3   = nn.BatchNorm2d(256)\n",
    "        self.conv4_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn4_4   = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Fourth residual block (Layer 4)\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn5_1   = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5_2   = nn.BatchNorm2d(512)\n",
    "        self.downsample5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        \n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5_3   = nn.BatchNorm2d(512)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn5_4   = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc      = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Layer 1\n",
    "        identity = x\n",
    "        out = self.conv2_1(x)\n",
    "        out = self.bn2_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2_2(out)\n",
    "        out = self.bn2_2(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity = out\n",
    "        out = self.conv2_3(out)\n",
    "        out = self.bn2_3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2_4(out)\n",
    "        out = self.bn2_4(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Layer 2\n",
    "        identity = out\n",
    "        out = self.conv3_1(out)\n",
    "        out = self.bn3_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out = self.bn3_2(out)\n",
    "        identity = self.downsample3(identity)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity = out\n",
    "        out = self.conv3_3(out)\n",
    "        out = self.bn3_3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv3_4(out)\n",
    "        out = self.bn3_4(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Layer 3\n",
    "        identity = out\n",
    "        out = self.conv4_1(out)\n",
    "        out = self.bn4_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv4_2(out)\n",
    "        out = self.bn4_2(out)\n",
    "        identity = self.downsample4(identity)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity = out\n",
    "        out = self.conv4_3(out)\n",
    "        out = self.bn4_3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv4_4(out)\n",
    "        out = self.bn4_4(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Layer 4\n",
    "        identity = out\n",
    "        out = self.conv5_1(out)\n",
    "        out = self.bn5_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv5_2(out)\n",
    "        out = self.bn5_2(out)\n",
    "        identity = self.downsample5(identity)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        identity = out\n",
    "        out = self.conv5_3(out)\n",
    "        out = self.bn5_3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv5_4(out)\n",
    "        out = self.bn5_4(out)\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Final layers\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145deae0-5f79-4d4d-9028-38f74b34a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18_unrolled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80984854-2087-499a-82de-5ca603a29af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18_unrolled(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2_4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (downsample3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3_4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (downsample4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn4_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn4_4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn5_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn5_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (downsample5): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn5_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn5_4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31decdb5-cbc1-4a35-9717-e04e94cc3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca463aa7-c527-47d0-8c2f-4679fc8a80c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet18_unrolled                        [16, 2]                   --\n",
      "├─Conv2d: 1-1                            [16, 64, 64, 64]          9,408\n",
      "├─BatchNorm2d: 1-2                       [16, 64, 64, 64]          128\n",
      "├─Conv2d: 1-3                            [16, 64, 32, 32]          36,864\n",
      "├─BatchNorm2d: 1-4                       [16, 64, 32, 32]          128\n",
      "├─Conv2d: 1-5                            [16, 64, 32, 32]          36,864\n",
      "├─BatchNorm2d: 1-6                       [16, 64, 32, 32]          128\n",
      "├─Conv2d: 1-7                            [16, 64, 32, 32]          36,864\n",
      "├─BatchNorm2d: 1-8                       [16, 64, 32, 32]          128\n",
      "├─Conv2d: 1-9                            [16, 64, 32, 32]          36,864\n",
      "├─BatchNorm2d: 1-10                      [16, 64, 32, 32]          128\n",
      "├─Conv2d: 1-11                           [16, 128, 16, 16]         73,728\n",
      "├─BatchNorm2d: 1-12                      [16, 128, 16, 16]         256\n",
      "├─Conv2d: 1-13                           [16, 128, 16, 16]         147,456\n",
      "├─BatchNorm2d: 1-14                      [16, 128, 16, 16]         256\n",
      "├─Sequential: 1-15                       [16, 128, 16, 16]         --\n",
      "│    └─Conv2d: 2-1                       [16, 128, 16, 16]         8,192\n",
      "│    └─BatchNorm2d: 2-2                  [16, 128, 16, 16]         256\n",
      "├─Conv2d: 1-16                           [16, 128, 16, 16]         147,456\n",
      "├─BatchNorm2d: 1-17                      [16, 128, 16, 16]         256\n",
      "├─Conv2d: 1-18                           [16, 128, 16, 16]         147,456\n",
      "├─BatchNorm2d: 1-19                      [16, 128, 16, 16]         256\n",
      "├─Conv2d: 1-20                           [16, 256, 8, 8]           294,912\n",
      "├─BatchNorm2d: 1-21                      [16, 256, 8, 8]           512\n",
      "├─Conv2d: 1-22                           [16, 256, 8, 8]           589,824\n",
      "├─BatchNorm2d: 1-23                      [16, 256, 8, 8]           512\n",
      "├─Sequential: 1-24                       [16, 256, 8, 8]           --\n",
      "│    └─Conv2d: 2-3                       [16, 256, 8, 8]           32,768\n",
      "│    └─BatchNorm2d: 2-4                  [16, 256, 8, 8]           512\n",
      "├─Conv2d: 1-25                           [16, 256, 8, 8]           589,824\n",
      "├─BatchNorm2d: 1-26                      [16, 256, 8, 8]           512\n",
      "├─Conv2d: 1-27                           [16, 256, 8, 8]           589,824\n",
      "├─BatchNorm2d: 1-28                      [16, 256, 8, 8]           512\n",
      "├─Conv2d: 1-29                           [16, 512, 4, 4]           1,179,648\n",
      "├─BatchNorm2d: 1-30                      [16, 512, 4, 4]           1,024\n",
      "├─Conv2d: 1-31                           [16, 512, 4, 4]           2,359,296\n",
      "├─BatchNorm2d: 1-32                      [16, 512, 4, 4]           1,024\n",
      "├─Sequential: 1-33                       [16, 512, 4, 4]           --\n",
      "│    └─Conv2d: 2-5                       [16, 512, 4, 4]           131,072\n",
      "│    └─BatchNorm2d: 2-6                  [16, 512, 4, 4]           1,024\n",
      "├─Conv2d: 1-34                           [16, 512, 4, 4]           2,359,296\n",
      "├─BatchNorm2d: 1-35                      [16, 512, 4, 4]           1,024\n",
      "├─Conv2d: 1-36                           [16, 512, 4, 4]           2,359,296\n",
      "├─BatchNorm2d: 1-37                      [16, 512, 4, 4]           1,024\n",
      "├─AdaptiveAvgPool2d: 1-38                [16, 512, 1, 1]           --\n",
      "├─Linear: 1-39                           [16, 2]                   1,026\n",
      "==========================================================================================\n",
      "Total params: 11,177,538\n",
      "Trainable params: 11,177,538\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 9.48\n",
      "==========================================================================================\n",
      "Input size (MB): 3.15\n",
      "Forward/backward pass size (MB): 207.62\n",
      "Params size (MB): 44.71\n",
      "Estimated Total Size (MB): 255.47\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet18_unrolled                        [16, 2]                   --\n",
       "├─Conv2d: 1-1                            [16, 64, 64, 64]          9,408\n",
       "├─BatchNorm2d: 1-2                       [16, 64, 64, 64]          128\n",
       "├─Conv2d: 1-3                            [16, 64, 32, 32]          36,864\n",
       "├─BatchNorm2d: 1-4                       [16, 64, 32, 32]          128\n",
       "├─Conv2d: 1-5                            [16, 64, 32, 32]          36,864\n",
       "├─BatchNorm2d: 1-6                       [16, 64, 32, 32]          128\n",
       "├─Conv2d: 1-7                            [16, 64, 32, 32]          36,864\n",
       "├─BatchNorm2d: 1-8                       [16, 64, 32, 32]          128\n",
       "├─Conv2d: 1-9                            [16, 64, 32, 32]          36,864\n",
       "├─BatchNorm2d: 1-10                      [16, 64, 32, 32]          128\n",
       "├─Conv2d: 1-11                           [16, 128, 16, 16]         73,728\n",
       "├─BatchNorm2d: 1-12                      [16, 128, 16, 16]         256\n",
       "├─Conv2d: 1-13                           [16, 128, 16, 16]         147,456\n",
       "├─BatchNorm2d: 1-14                      [16, 128, 16, 16]         256\n",
       "├─Sequential: 1-15                       [16, 128, 16, 16]         --\n",
       "│    └─Conv2d: 2-1                       [16, 128, 16, 16]         8,192\n",
       "│    └─BatchNorm2d: 2-2                  [16, 128, 16, 16]         256\n",
       "├─Conv2d: 1-16                           [16, 128, 16, 16]         147,456\n",
       "├─BatchNorm2d: 1-17                      [16, 128, 16, 16]         256\n",
       "├─Conv2d: 1-18                           [16, 128, 16, 16]         147,456\n",
       "├─BatchNorm2d: 1-19                      [16, 128, 16, 16]         256\n",
       "├─Conv2d: 1-20                           [16, 256, 8, 8]           294,912\n",
       "├─BatchNorm2d: 1-21                      [16, 256, 8, 8]           512\n",
       "├─Conv2d: 1-22                           [16, 256, 8, 8]           589,824\n",
       "├─BatchNorm2d: 1-23                      [16, 256, 8, 8]           512\n",
       "├─Sequential: 1-24                       [16, 256, 8, 8]           --\n",
       "│    └─Conv2d: 2-3                       [16, 256, 8, 8]           32,768\n",
       "│    └─BatchNorm2d: 2-4                  [16, 256, 8, 8]           512\n",
       "├─Conv2d: 1-25                           [16, 256, 8, 8]           589,824\n",
       "├─BatchNorm2d: 1-26                      [16, 256, 8, 8]           512\n",
       "├─Conv2d: 1-27                           [16, 256, 8, 8]           589,824\n",
       "├─BatchNorm2d: 1-28                      [16, 256, 8, 8]           512\n",
       "├─Conv2d: 1-29                           [16, 512, 4, 4]           1,179,648\n",
       "├─BatchNorm2d: 1-30                      [16, 512, 4, 4]           1,024\n",
       "├─Conv2d: 1-31                           [16, 512, 4, 4]           2,359,296\n",
       "├─BatchNorm2d: 1-32                      [16, 512, 4, 4]           1,024\n",
       "├─Sequential: 1-33                       [16, 512, 4, 4]           --\n",
       "│    └─Conv2d: 2-5                       [16, 512, 4, 4]           131,072\n",
       "│    └─BatchNorm2d: 2-6                  [16, 512, 4, 4]           1,024\n",
       "├─Conv2d: 1-34                           [16, 512, 4, 4]           2,359,296\n",
       "├─BatchNorm2d: 1-35                      [16, 512, 4, 4]           1,024\n",
       "├─Conv2d: 1-36                           [16, 512, 4, 4]           2,359,296\n",
       "├─BatchNorm2d: 1-37                      [16, 512, 4, 4]           1,024\n",
       "├─AdaptiveAvgPool2d: 1-38                [16, 512, 1, 1]           --\n",
       "├─Linear: 1-39                           [16, 2]                   1,026\n",
       "==========================================================================================\n",
       "Total params: 11,177,538\n",
       "Trainable params: 11,177,538\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 9.48\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 207.62\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 255.47\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "summary(model, input_size=(batch_size, 3, 128, 128), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0bde6-c9b0-4b8e-a612-7594ea09b9fb",
   "metadata": {},
   "source": [
    "In deeper versions of ResNet, like ResNet-50, ResNet-101, and ResNet-152, a more efficient type of residual block is used, known as the bottleneck block. The bottleneck block allows the network to remain computationally efficient, even as the number of layers increases.\n",
    "\n",
    "![ResNet blocks](figures/Structure-of-basic-blocks-from-our-ResNet18-Employ-structure-a-in-cases-where-the_Q320.jpg)\n",
    "\n",
    "![ResNet blocks](figures/resnet_blocks.png)\n",
    "\n",
    "![ResNet blocks](figures/ResNet50-architecture-built-using-bottleneck-blocks-of-aIdentity-shortcut-and-1403887865.jpg)\n",
    "\n",
    "**Why Bottleneck Blocks?**\n",
    "\n",
    "As networks grow deeper, computational costs (in terms of memory and processing power) become a concern. For deeper networks to be practical, they need to balance computational complexity with their ability to learn complex features. The bottleneck block addresses this issue by reducing the number of parameters while still allowing for deep architectures.\n",
    "\n",
    "**Structure of a Bottleneck Block**\n",
    "\n",
    "A bottleneck block consists of three convolutional layers:\n",
    "\n",
    "* 1x1 Convolution (Compression Layer): This reduces the number of channels (i.e., the feature map's depth). This step is called \"dimensionality reduction\" because it compresses the input feature map into a smaller, more manageable size. It reduces computational cost and speeds up training.\n",
    "\n",
    "* 3x3 Convolution (Processing Layer): This is the core processing layer, where the actual feature extraction happens. It operates on the reduced number of channels from the previous layer, which makes this step computationally efficient.\n",
    "\n",
    "* 1x1 Convolution (Expansion Layer): This layer increases the number of channels back to the original dimension. It restores the depth of the feature map, ensuring that no information is lost while keeping the computational load lighter.\n",
    "\n",
    "* In between each of these convolutional layers, batch normalization and ReLU activation functions are applied to normalize and activate the output.\n",
    "\n",
    "**Why the Bottleneck Design Works and the Importance of Feature Selection**\n",
    "\n",
    "The bottleneck block in ResNet operates by compressing the feature representation before performing more expensive operations, then expanding it back to the original size. This compression-expansion strategy reduces the number of parameters in the network, making it more computationally efficient while retaining essential information.\n",
    "\n",
    "For example, in a traditional convolutional layer, if the input has 256 channels, a 3x3 convolution would need to process all 256 channels. In the bottleneck block, the first 1x1 convolution reduces the number of channels to, say, 64. The 3x3 convolution then operates on this reduced set, significantly lowering the computational load. Afterward, the final 1x1 convolution restores the channel size back to 256.\n",
    "\n",
    "However, the 1x1 convolution used for dimensionality reduction (compression) is not merely a way to decrease the number of channels. It plays a crucial role in selecting and extracting the most important features from the input. The learned filters in this layer focus on preserving the essential and informative aspects of the input, ensuring that even with fewer channels, critical features are passed to the 3x3 convolution. This prioritization allows the network to avoid processing irrelevant or redundant information, making the deeper layers more effective and efficient.\n",
    "\n",
    "Thus, the bottleneck block’s design combines both computational efficiency and intelligent feature selection, ensuring that the network can process fewer channels without sacrificing important information, ultimately reducing the loss and improving performance.\n",
    "\n",
    "**Skip Connections in Bottleneck Blocks**\n",
    "\n",
    "Like the basic residual block, the bottleneck block also uses skip connections to connect the input to the output. If the input and output dimensions differ (as is often the case due to the dimensionality reduction in the block), a 1x1 convolution is applied to the input in the skip connection to match the dimensions before adding the input to the output.\n",
    "\n",
    "**Benefits of Bottleneck Blocks**\n",
    "\n",
    "* Efficiency: By using 1x1 convolutions to reduce and then restore the number of channels, bottleneck blocks reduce the computational complexity of the network without sacrificing performance.\n",
    "\n",
    "* Depth without Degradation: Bottleneck blocks allow for deeper networks (50, 101, or 152 layers) by avoiding the vanishing gradient and degradation problems through residual learning.\n",
    "\n",
    "* Better Feature Extraction: Even with fewer parameters, bottleneck blocks can still capture complex features, thanks to the reduced number of channels in the middle of the block.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The 1x1 convolution in bottleneck blocks compresses feature maps, but the learned filters in this layer ensure that only the most important features are selected.\n",
    "\n",
    "This process allows the network to focus on critical information, improving generalization and preventing overfitting.\n",
    "\n",
    "The compressed features preserve essential information, which helps reduce loss without sacrificing performance, even in very deep networks.\n",
    "This balance between efficient computation and information preservation is one of the reasons ResNet's bottleneck blocks are so powerful, especially in deep architectures like ResNet-50 and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c151a1b-7c5d-4e84-9ec5-feaeac6f7320",
   "metadata": {},
   "source": [
    "# Batch Normalization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe575e-9ff4-4d27-885b-e97594e7e369",
   "metadata": {},
   "source": [
    "As you probably have noted, dropout layer is not used in ResNet. In deep networks, dropout can make the gradient flow unstable due to introduced noise, which is counterproductive when you're trying to address vanishing or exploding gradients.\n",
    "\n",
    "Batch normalization is a technique used in deep learning to improve the training of neural networks by normalizing the inputs of each layer. Introduced by Sergey Ioffe and Christian Szegedy in 2015, this method addresses several problems that arise during training, particularly the issue of internal covariate shift. Normalizing the inputs of each layer helps to stabilize the training process by ensuring that there is a consistent mean and variance. This stabilization reduces the risk of vanishing or exploding gradients, making it easier to train very deep networks, which can have hundreds of layers.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "* Internal Covariate Shift:\n",
    "\n",
    "During training, the distribution of inputs to each layer in a neural network changes as the model's parameters (weights and biases) are updated. This phenomenon is called internal covariate shift.\n",
    "It slows down training because each layer has to constantly adapt to changing inputs from the previous layers, making the optimization process harder.\n",
    "\n",
    "* Normalization:\n",
    "\n",
    "Normalization is the process of transforming the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "Batch normalization normalizes the inputs to each layer in a neural network so that they maintain a stable distribution. This reduces the internal covariate shift and allows the network to converge faster.\n",
    "\n",
    "**How Batch Normalization Works**\n",
    "\n",
    "Batch normalization operates on mini-batches of data during training. The steps involved are as follows:\n",
    "\n",
    "* Calculate the Mean and Variance:\n",
    "\n",
    "   For each mini-batch of data, compute the mean $\\mu_B$ and variance $\\sigma_B^2$ of the input values to a layer.\n",
    "\n",
    "   $$ \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$\n",
    "\n",
    "   $$ \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2 $$\n",
    "\n",
    "   Where:\n",
    "   - $m$ is the batch size.\n",
    "   - $x_i$ is the input to the layer for the $i$-th example.\n",
    "\n",
    "\n",
    "* Normalize the Inputs:\n",
    "\n",
    "   Normalize each input by subtracting the mean and dividing by the standard deviation:\n",
    "\n",
    "   $$ \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$\n",
    "\n",
    "   Here, $\\epsilon$ is a small constant added to avoid division by zero.\n",
    "\n",
    "\n",
    "* Scale and Shift:\n",
    "\n",
    "   To allow the model to represent a wide range of activations, two learnable parameters, $\\gamma$ and $\\beta$, are introduced. These parameters scale and shift the normalized output:\n",
    "\n",
    "   $$ y_i = \\gamma \\hat{x}_i + \\beta $$\n",
    "\n",
    "   This step ensures that even though the inputs are normalized, the network can learn to undo this normalization if needed.\n",
    "\n",
    "\n",
    "* Use in Both Training and Inference:\n",
    "\n",
    "   During training, the mean and variance are computed for each batch. During inference, fixed running averages of the mean and variance (calculated during training) are used to ensure consistency.\n",
    "\n",
    "\n",
    "**Advantages of Batch Normalization**\n",
    "\n",
    "* Faster Training: By normalizing inputs, batch normalization allows for higher learning rates without the risk of divergence. This speeds up the convergence of the model during training.\n",
    "\n",
    "* Reduces Dependence on Initialization: Deep networks are sensitive to the initialization of weights. Batch normalization makes the training less sensitive to initialization, which means that even with poor initial weights, the model can still converge.\n",
    "\n",
    "* Acts as a Regularizer: Batch normalization introduces some noise into the training process because it normalizes based on mini-batches, which vary slightly from each other. This noise can have a slight regularization effect, similar to dropout, reducing overfitting.\n",
    "\n",
    "* Eases Gradient Flow: Normalizing inputs at each layer helps keep the gradient magnitudes stable, reducing the risk of exploding or vanishing gradients. This is especially important for training deep networks.\n",
    "\n",
    "**Where Batch Normalization is Applied**\n",
    "\n",
    "Batch normalization is typically applied before (most common) the activation function in a neural network layer. It could be applied and after, but the exact placement varies by implementation.\n",
    "\n",
    "**Limitations of Batch Normalization**\n",
    "\n",
    "* Dependence on Batch Size: Batch normalization's performance can degrade if the batch size is too small because the mean and variance estimates become noisy with smaller batches.\n",
    "\n",
    "* Not Ideal for All Types of Data: For certain types of models, like Recurrent Neural Networks (RNNs), where the sequence length varies, batch normalization might not be the best choice. Instead, techniques like Layer Normalization or Group Normalization are more suited for such architectures.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Batch normalization is a powerful tool that has become a standard in modern deep learning architectures. By reducing internal covariate shift, improving gradient flow, and acting as a form of regularization, it enables faster and more reliable training of deep neural networks. It has proven especially useful in very deep networks, where training can be unstable or slow without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafd372-7f49-4785-83fd-fa147bfcb72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
