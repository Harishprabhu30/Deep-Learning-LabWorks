{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fd6f17-a272-4bf0-ad26-f8c0692a5c6f",
   "metadata": {},
   "source": [
    "# Some details on Pytorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7ef94-6c09-408f-baeb-7a1e62cb40f9",
   "metadata": {},
   "source": [
    "In PyTorch, tensors are the primary data structure, similar to arrays or matrices, and they store data for neural network operations. However, unlike regular arrays, PyTorch tensors have additional fields and capabilities to support automatic differentiation, which is crucial for training neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ed934-347c-4600-a51a-00a488c13953",
   "metadata": {},
   "source": [
    "## Basic Fields in a PyTorch Tensor\n",
    "\n",
    "A PyTorch tensor contains the following fundamental fields:\n",
    "\n",
    "- **Data (`data`)**: This is the actual numerical data of the tensor, represented as a multidimensional array. The data can be any numeric type (float, integer, etc.), depending on the tensor's purpose.\n",
    "\n",
    "- **Data Type (`dtype`)**: This indicates the type of data stored in the tensor, such as `torch.float32`, `torch.int64`, etc. `dtype` is crucial because it affects the precision and memory requirements of the tensor.\n",
    "\n",
    "- **Shape (`size()` or `shape`)**: This defines the dimensions of the tensor, like `(3, 3)` for a 3x3 matrix. Knowing the shape is essential for operations on tensors, as operations typically require tensors with compatible shapes.\n",
    "\n",
    "- **Device (`device`)**: This specifies where the tensor is stored, either on the CPU (`torch.device(\"cpu\")`) or GPU (`torch.device(\"cuda\")`). When you want to perform computations on the GPU for faster performance, the tensor needs to be on a CUDA device.\n",
    "\n",
    "## Fields for Gradient Calculations in PyTorch Tensors\n",
    "\n",
    "In addition to the basic fields, tensors involved in training deep learning models have additional fields related to automatic differentiation (autograd), which PyTorch uses to compute gradients. These fields are only present if `requires_grad=True` when the tensor is created. Here’s an overview of these fields:\n",
    "\n",
    "- **`requires_grad`**: This boolean flag indicates whether PyTorch should track operations on the tensor for gradient computation. If `requires_grad=True`, PyTorch records all operations on this tensor to compute its gradients later. This is essential for parameters (weights and biases) in neural networks, as it allows gradients to be calculated during backpropagation.\n",
    "\n",
    "- **`grad`**: This is the tensor that holds the gradients of a tensor with respect to some scalar (typically the loss function in a neural network). After calling `backward()` on a loss tensor, the `.grad` field of any tensor with `requires_grad=True` will contain the computed gradient.\n",
    "\n",
    "  - Example: If you have a loss function `L` that depends on tensor `x`, then after `L.backward()`, `x.grad` will hold the value of \\( \\frac{dL}{dx} \\).\n",
    "\n",
    "- **`grad_fn`**: This attribute points to a `Function` that has created the tensor (if it was created by an operation that requires gradients). `grad_fn` is part of the autograd system that stores the operation history and allows PyTorch to trace back through the operations for gradient calculation.\n",
    "\n",
    "  - For instance, if `z = x + y` where both `x` and `y` have `requires_grad=True`, then `z.grad_fn` will be `AddBackward0`, showing that `z` was created by an addition operation.\n",
    "\n",
    "## The Autograd Computational Graph\n",
    "\n",
    "To understand how gradients are computed in PyTorch, it’s helpful to look at the autograd computational graph. When `requires_grad=True` is set, PyTorch builds a dynamic computational graph, with nodes representing operations and edges representing data flows. Here’s a basic outline:\n",
    "\n",
    "- **Forward Pass**: When you perform operations on tensors, PyTorch builds a graph where each tensor stores its `grad_fn`. The `grad_fn` links to the function that created it, and any subsequent operations will build on this.\n",
    "\n",
    "- **Backward Pass**: When you call `.backward()` on a scalar tensor (typically a loss), PyTorch traverses this graph in reverse, computing gradients at each step using the chain rule. The result of each gradient calculation is stored in the `.grad` field of the involved tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e69758-65d3-4b4a-a169-9c5072601da2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Field           | Description                                                                                              |\n",
    "|-----------------|----------------------------------------------------------------------------------------------------------|\n",
    "| `data`          | Contains the raw data of the tensor.                                                                     |\n",
    "| `dtype`         | Specifies the data type (e.g., `torch.float32`).                                                         |\n",
    "| `shape`         | The dimensions of the tensor.                                                                            |\n",
    "| `device`        | Indicates if the tensor is on CPU or GPU.                                                                |\n",
    "| `requires_grad` | If `True`, enables automatic differentiation for gradient calculation.                                   |\n",
    "| `grad`          | Stores the computed gradients (only if `requires_grad=True`).                                            |\n",
    "| `grad_fn`       | Points to the function that created the tensor, used by autograd to build the computational graph.       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6053e7-3ca1-436a-9e50-377ee41fa88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Tensor dtype: torch.float32\n",
      "Tensor shape: torch.Size([2, 2])\n",
      "Tensor device: cpu\n",
      "Tensor requires_grad: False\n",
      "Tensor grad: None\n",
      "Tensor grad_fn: None\n",
      "\n",
      "\n",
      "Tensor2 shape: torch.Size([3])\n",
      "Tensor2 device: cpu\n",
      "Tensor2 requires_grad: True\n",
      "Tensor2 grad: None\n",
      "Tensor2 grad_fn: None\n",
      "Result after operation (result = tensor2 * 2 + 1): tensor([3., 5., 7.], grad_fn=<AddBackward0>)\n",
      "Result's grad_fn: <AddBackward0 object at 0x000001CB53EEA2F0>\n",
      "Result shape: torch.Size([3])\n",
      "Result device: cpu\n",
      "\n",
      "\n",
      "y = x^2: tensor(9., grad_fn=<PowBackward0>)\n",
      "Gradient dy/dx (stored in x.grad): tensor(6.)\n",
      "\n",
      "\n",
      "d = c + a: tensor(12., grad_fn=<AddBackward0>)\n",
      "e = d^2: tensor(144., grad_fn=<PowBackward0>)\n",
      "e's grad_fn: <PowBackward0 object at 0x000001CB53EEA2F0>\n",
      "Gradient of a: tensor(144.)\n",
      "Gradient of b: tensor(48.)\n",
      "\n",
      "\n",
      "Result with no gradient tracking: tensor(15.)\n",
      "f requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example 1: Basic Properties of a Tensor\n",
    "# Create a tensor with specified dtype and device\n",
    "tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32, device=\"cpu\")\n",
    "print(\"Tensor data:\\n\", tensor1.data)\n",
    "print(\"Tensor dtype:\", tensor1.dtype)\n",
    "print(\"Tensor shape:\", tensor1.shape)\n",
    "print(\"Tensor device:\", tensor1.device)\n",
    "print(\"Tensor requires_grad:\", tensor1.requires_grad)\n",
    "print(\"Tensor grad:\", tensor1.grad)\n",
    "print(\"Tensor grad_fn:\", tensor1.grad_fn)\n",
    "print(\"\\n\")  # Separator for readability\n",
    "\n",
    "# Example 2: Enabling Gradient Calculations\n",
    "# Create a tensor with requires_grad=True\n",
    "tensor2 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"Tensor2 shape:\", tensor2.shape)\n",
    "print(\"Tensor2 device:\", tensor2.device)\n",
    "print(\"Tensor2 requires_grad:\", tensor2.requires_grad)\n",
    "print(\"Tensor2 grad:\", tensor2.grad)\n",
    "print(\"Tensor2 grad_fn:\", tensor2.grad_fn)\n",
    "\n",
    "# Perform an operation on tensor2\n",
    "result = tensor2 * 2 + 1\n",
    "print(\"Result after operation (result = tensor2 * 2 + 1):\", result)\n",
    "print(\"Result's grad_fn:\", result.grad_fn)  # Shows the function that created the result\n",
    "print(\"Result shape:\", result.shape)\n",
    "print(\"Result device:\", result.device)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 3: Calculating Gradients\n",
    "# Define a new tensor and a simple function with it\n",
    "x = torch.tensor(3.0, requires_grad=True)  # Create tensor with requires_grad=True\n",
    "y = x ** 2  # Define a function y = x^2\n",
    "print(\"y = x^2:\", y)\n",
    "\n",
    "# Calculate the gradient of y with respect to x\n",
    "y.backward()  # This computes dy/dx and stores it in x.grad\n",
    "print(\"Gradient dy/dx (stored in x.grad):\", x.grad)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 4: More Complex Graphs and Multiple Operations\n",
    "# Create two tensors with requires_grad=True\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "# Perform a series of operations\n",
    "c = a * b   # Multiplication\n",
    "d = c + a   # Addition\n",
    "e = d ** 2  # Power operation\n",
    "\n",
    "print(\"d = c + a:\", d)\n",
    "print(\"e = d^2:\", e)\n",
    "print(\"e's grad_fn:\", e.grad_fn)  # Shows that e was created by a PowBackward function\n",
    "\n",
    "# Backpropagate through this complex graph\n",
    "e.backward()  # Calculate gradients\n",
    "print(\"Gradient of a:\", a.grad)  # Prints da/de\n",
    "print(\"Gradient of b:\", b.grad)  # Prints db/de\n",
    "print(\"\\n\")\n",
    "\n",
    "# Example 5: Disabling Gradient Tracking\n",
    "# Useful for inference, where we don't need gradients to save memory\n",
    "with torch.no_grad():\n",
    "    f = a * b + b\n",
    "    print(\"Result with no gradient tracking:\", f)\n",
    "    print(\"f requires_grad:\", f.requires_grad)  # Should be False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c711f-f3ba-4eb7-a82b-69de58c4a726",
   "metadata": {},
   "source": [
    "## Calculations on tensors outside of PyTorch\n",
    "\n",
    "To perform calculations outside of PyTorch, you can convert PyTorch tensors to other commonly used formats, such as NumPy arrays or Python lists.\n",
    "\n",
    "**Converting a PyTorch Tensor to a NumPy Array**\n",
    "\n",
    "NumPy arrays are widely used in Python for numerical calculations, and converting PyTorch tensors to NumPy arrays is straightforward. You can use the `.numpy()` method to achieve this.\n",
    "\n",
    "**Important Consideration**\n",
    "\n",
    "* Shared Memory: The PyTorch tensor and the NumPy array will share the same underlying memory if the tensor is on the CPU. This means that modifying the NumPy array will also change the PyTorch tensor, and vice versa.\n",
    "\n",
    "* GPU Tensors: If the tensor is on a GPU (i.e., `device='cuda'`), you need to first move it to the CPU before converting it to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e965f9-ceba-4e16-a058-bedab06bcf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1., 2., 3.])\n",
      "NumPy array: [1. 2. 3.]\n",
      "Type of numpy_array: <class 'numpy.ndarray'>\n",
      "Tensor: tensor([10.,  2.,  3.])\n",
      "NumPy array: [10.  2.  3.]\n",
      "NumPy array: [1. 2. 3.]\n",
      "Type of numpy_array: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Convert to a NumPy array\n",
    "numpy_array = tensor.numpy()\n",
    "print(\"Tensor:\", tensor)\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "print(\"Type of numpy_array:\", type(numpy_array))\n",
    "\n",
    "# change value in numpy array (memory is shared so changes will happen in tensor too)\n",
    "numpy_array[0] = 10\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "print(\"Tensor:\", tensor)\n",
    "\n",
    "tensor_gpu = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n",
    "numpy_array = tensor_gpu.cpu().numpy()  # Move to CPU first, then convert\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "print(\"Type of numpy_array:\", type(numpy_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7f806-561d-4f34-b87f-4327862592c4",
   "metadata": {},
   "source": [
    "**Converting a PyTorch Tensor to a Python List**\n",
    "\n",
    "If you need a Python list (e.g., for simpler data structures or exporting data), you can use the `.tolist()` method. This method works on both CPU and GPU tensors, but will always return a list of standard Python types, detached from the original PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8d9e4b-5bb6-4e6f-a7d2-3153deb69f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list: [[1.0, 2.0], [3.0, 4.0]]\n",
      "Type of python_list: <class 'list'>\n",
      "Python list: [[1.0, 5.0], [3.0, 4.0]]\n",
      "Tensor: tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Convert to a Python list\n",
    "python_list = tensor.tolist()\n",
    "print(\"Python list:\", python_list)\n",
    "print(\"Type of python_list:\", type(python_list))\n",
    "\n",
    "# Memory not shared\n",
    "python_list[0][1] = 5.0\n",
    "print(\"Python list:\", python_list)\n",
    "print(\"Tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3087d9c-0758-46b6-9038-e4e3d4dbc4ab",
   "metadata": {},
   "source": [
    "**Detaching a Tensor Before Conversion**\n",
    "\n",
    "If the tensor requires gradients (i.e., `requires_grad=True`), you may want to detach it before converting to avoid unintended modifications to the autograd computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c128fb6e-c715-4d98-9142-23d6091b413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1., 2., 3.], requires_grad=True)\n",
      "Detached NumPy array: [1. 2. 3.]\n",
      "Detached NumPy array: [10.  2.  3.]\n",
      "Tensor: tensor([10.,  2.,  3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with requires_grad=True\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Detach the tensor, then convert to a NumPy array\n",
    "numpy_array = tensor.detach().numpy()\n",
    "print(\"Tensor:\", tensor)\n",
    "print(\"Detached NumPy array:\", numpy_array)\n",
    "\n",
    "# Memory is shared, so changes in numpy are reflected in tensor\n",
    "numpy_array[0] = 10\n",
    "print(\"Detached NumPy array:\", numpy_array)\n",
    "print(\"Tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2f7df-2b79-42d7-8e3e-a8cbb6d5cdd8",
   "metadata": {},
   "source": [
    "The `.item()` method in PyTorch is used to extract a single value from a one-element tensor and convert it to a standard Python scalar (like an `int` or `float`). This can be helpful when you want to work with a single numerical value outside of PyTorch.\n",
    "\n",
    "* One-Element Tensor: `.item()` only works on tensors with a single value (a one-element tensor). It will raise an error if used on tensors with more than one element.\n",
    "* Gradient or Loss Values: `.item()` is often used to log or store scalar values like loss or accuracy, which are typically single numbers.\n",
    "* Python Functions: If you need to use a tensor value in a Python function that doesn’t accept PyTorch tensors, `.item()` converts it to a compatible scalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4c158-5278-4a79-8326-4b48568daf48",
   "metadata": {},
   "source": [
    "## In-Place vs Out-of-Place operation\n",
    "\n",
    "In PyTorch, the difference between in-place and out-of-place operations for ReLU (Rectified Linear Unit) revolves around whether the operation modifies the input tensor directly (in-place) or creates a new tensor with the result (out-of-place).\n",
    "\n",
    "**Out-of-Place ReLU**\n",
    "\n",
    "The out-of-place ReLU function in PyTorch does not modify the input tensor itself. Instead, it returns a new tensor with the result of applying the ReLU operation, leaving the original tensor unchanged.\n",
    "\n",
    "Syntax: `output = torch.relu(input) or output = nn.ReLU()(input)`\n",
    "\n",
    "Memory: Out-of-place operations consume additional memory, as a new tensor is created to store the result.\n",
    "Use Case: Out-of-place operations are generally safer, especially when the original input tensor is needed later in the computation graph for backward passes or further operations.\n",
    "\n",
    "**In-Place ReLU**\n",
    "\n",
    "In-place ReLU modifies the input tensor directly. It replaces the negative values with zero in the same tensor, avoiding the creation of a new tensor.\n",
    "\n",
    "Syntax: `input.relu_() or nn.ReLU(inplace=True)(input)`\n",
    "\n",
    "Memory: In-place operations are more memory-efficient as they don’t create a new tensor. This can be beneficial when working with large tensors.\n",
    "Use Case: In-place ReLU is useful when memory optimization is crucial. However, it can complicate backpropagation if the original tensor values are needed later in the computation.\n",
    "\n",
    "**Points to Consider**\n",
    "\n",
    "* Backward Compatibility: Some operations in the backward pass may require the original values. In-place operations can lead to issues or errors in backward computations if they overwrite values needed for gradients.\n",
    "\n",
    "* Performance vs. Safety: In-place operations can save memory and sometimes improve speed, but they can reduce safety due to possible overwrites of intermediate values in the computation graph.\n",
    "\n",
    "In summary, in-place ReLU modifies the original tensor directly, saving memory, while out-of-place ReLU leaves the original tensor unchanged, ensuring compatibility with the backward pass and allowing safer handling in complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dab4c4c-b7a0-462d-9976-cfca0bae0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  2., -3.,  4.])\n",
      "tensor([0., 2., 0., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.tensor([-1.0, 2.0, -3.0, 4.0])\n",
    "output = torch.relu(input)  # Does not modify `input`\n",
    "print(input)   # tensor([-1.0, 2.0, -3.0, 4.0])\n",
    "print(output)  # tensor([0.0, 2.0, 0.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5719289-c076-4527-bc0a-cce2119febee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  2., -3.,  4.])\n",
      "Out-of-Place\n",
      "tensor([-1.,  2., -3.,  4.])\n",
      "tensor([0., 2., 0., 4.])\n",
      "In-Place\n",
      "tensor([0., 2., 0., 4.])\n",
      "tensor([0., 2., 0., 4.])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([-1.0, 2.0, -3.0, 4.0])\n",
    "print(input)\n",
    "#input.relu_()  # Modifies `input` directly\n",
    "relu_outplace = torch.nn.ReLU(inplace=False)\n",
    "relu_inplace = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "print(\"Out-of-Place\")\n",
    "output = relu_outplace(input)\n",
    "print(input)\n",
    "print(output)\n",
    "\n",
    "print(\"In-Place\")\n",
    "output = relu_inplace(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0daba2d-85a9-499a-a50c-422a5626e2c2",
   "metadata": {},
   "source": [
    "## `.detach()`\n",
    "In PyTorch, `.detach()` is a method used to separate a tensor from the computation graph. This operation is particularly useful when working with tensors that require gradients but need to be treated as constant (non-trainable) in certain parts of the code.\n",
    "\n",
    "**Functionality of `.detach()`**\n",
    "\n",
    "* Breaks the Computation Graph: When you call `.detach()` on a tensor, it creates a new tensor that shares the same data as the original tensor but does not require gradients. This \"detached\" tensor is no longer tracked by PyTorch's autograd, which means changes to it will not affect the original tensor, and backpropagation will not compute gradients for it.\n",
    "\n",
    "* Prevents Gradient Flow: Detaching a tensor effectively stops gradient flow at that point. This is useful when you want to use the value of a tensor in calculations without affecting gradient calculations in the computation graph.\n",
    "  \n",
    "Use Cases of `.detach()`\n",
    "\n",
    "* Freeze Parts of the Model: When fine-tuning a model, you might want to \"freeze\" certain layers so their weights remain unchanged during backpropagation. Detaching the output of such layers ensures they won’t contribute gradients.\n",
    "\n",
    "* Intermediate Calculations: Sometimes, you need intermediate values for calculations that you don’t want to impact the model’s learning. Detaching these tensors can prevent them from accumulating unnecessary gradients.\n",
    "\n",
    "* Memory Optimization: Detaching tensors that don’t need gradients reduces the memory overhead, as PyTorch will not store computational history for these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c814c73-61d1-464f-805b-908099e53870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor that requires gradients\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform a calculation that creates a computation graph\n",
    "y = x * 3  # y is now part of the computation graph\n",
    "\n",
    "# Detach `y` from the graph\n",
    "z = y.detach()  # z is a detached version of y\n",
    "\n",
    "# Continue with other operations\n",
    "w = z * 2  # w is independent of the computation graph\n",
    "\n",
    "# Backpropagation through `y`, but `z` and `w` won't affect it\n",
    "y.sum().backward()  # Only `x` will have gradients calculated, not `z` or `w`\n",
    "\n",
    "print(x.grad)  # Prints tensor([3., 3.]) based on `y = x * 3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1797ff-3fef-4dbd-be4d-377833fe34ff",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "`y` depends on `x` and requires gradients, so backpropagation will compute `x.grad`.\n",
    "\n",
    "`z` is detached, meaning it does not track gradients, and operations on `z` (like `w = z * 2`) will not create a computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75b51d-739a-451b-8d19-41f1086a90c7",
   "metadata": {},
   "source": [
    "**Using `.detach()` Without Sharing Data**\n",
    "\n",
    "* Detach with `.detach()`: This creates a new tensor that is detached from the computational graph, so it won’t track gradients. However, it still shares data with the original tensor.\n",
    "* Clone with `.clone()`: By chaining `.clone()` after `.detach()`, you create a new, independent copy of the tensor’s data in memory. Now, the detached tensor won’t be linked to the original tensor in any way.\n",
    "\n",
    "* Memory Independence: If you only use `.detach()`, any modification to the detached tensor will affect the original tensor because they share memory.\n",
    "* Gradient-Free Copy: This approach is useful for logging or manipulating data without affecting the original tensor, especially when working with intermediate results during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae08ae-c7c1-4c87-962c-23e627fa13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([1., 2., 3.], requires_grad=True)\n",
      "Detached tensor: tensor([1., 2., 3.])\n",
      "\n",
      "After modifying the detached tensor:\n",
      "Original tensor: tensor([1., 2., 3.], requires_grad=True)\n",
      "Detached tensor: tensor([10.,  2.,  3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor with requires_grad=True\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Detach and clone to avoid shared data\n",
    "detached_tensor = tensor.detach().clone()\n",
    "\n",
    "# Now detached_tensor and tensor do not share data\n",
    "print(\"Original tensor:\", tensor)\n",
    "print(\"Detached tensor:\", detached_tensor)\n",
    "\n",
    "# Modify the detached tensor\n",
    "detached_tensor[0] = 10.0\n",
    "print(\"\\nAfter modifying the detached tensor:\")\n",
    "print(\"Original tensor:\", tensor)  # Remains unchanged\n",
    "print(\"Detached tensor:\", detached_tensor)  # Shows the modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce120e-3aac-47c7-9519-88830679525d",
   "metadata": {},
   "source": [
    "**Key Points**\n",
    "\n",
    "`.detach()` is useful for stopping gradients, saving memory, and isolating parts of a model or computation.\n",
    "The detached tensor shares the same underlying data but does not require gradients.\n",
    "\n",
    "It’s different from `.requires_grad_(False)`, which changes a tensor’s `requires_grad` attribute entirely. Detach creates a separate tensor that does not track gradients but does not change the original tensor's attributes.\n",
    "\n",
    "Use `.detach()` to remove a tensor from the computation graph.\n",
    "Chain `.clone()` to ensure the detached tensor has its own independent memory, so it doesn’t affect the original tensor.\n",
    "\n",
    "In summary, `.detach()` is a powerful tool for selectively stopping gradient computations and isolating parts of a tensor's data for non-gradient uses in  PyTorch workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d687b-0925-44f5-862e-2d0422ec2fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
